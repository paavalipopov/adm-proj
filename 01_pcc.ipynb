{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271bd029",
   "metadata": {},
   "source": [
    "# requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b50e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "from src.load_data import load_data\n",
    "\n",
    "fbirn_data, demographics = load_data()\n",
    "timeseries = fbirn_data[\"data\"]\n",
    "diagnoses = fbirn_data[\"diags\"]\n",
    "sexes = fbirn_data[\"sexes\"]\n",
    "ages = fbirn_data[\"ages\"]\n",
    "\n",
    "print(demographics)\n",
    "print(f\"# subjects: {timeseries.shape[0]}, # timepoints: {timeseries.shape[1]}, # features: {timeseries.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a9b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ica_coords = pd.read_csv(\"data/ICN_coordinates.csv\")\n",
    "domains = ica_coords[\"Domain\"]\n",
    "# update nans with previous value\n",
    "domains = domains.fillna(method='ffill')\n",
    "domains = np.asarray(domains.tolist())\n",
    "\n",
    "change_idx = np.flatnonzero(np.r_[True, domains[1:] != domains[:-1]])\n",
    "# change_idx marks the start index of each group\n",
    "starts = change_idx\n",
    "# compute ends (inclusive) indices for each group\n",
    "ends = np.r_[starts[1:] - 1, domains.size - 1]\n",
    "centers = ((starts + ends) / 2.0).tolist()\n",
    "# boundaries are positions between pixels: (end + 0.4) for each group except last\n",
    "boundaries = (ends[:-1] + 0.4).tolist()\n",
    "\n",
    "group_names_full = [domains[s] for s in starts]\n",
    "group_names = [\"SC\", \"AU\", \"SM\", \"VIS\", \"CC\", \"DM\", \"CB\"]  # short names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac171535",
   "metadata": {},
   "source": [
    "# 0. First milestone\n",
    "- Write a general setup for the experiments\n",
    "- Find important features using stattests \n",
    "- Train classifiers, inspect the features that they found important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceb8bd1",
   "metadata": {},
   "source": [
    "## derive PCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfff4dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import corrcoef_batch\n",
    "\n",
    "pcc_matrices = corrcoef_batch(timeseries)\n",
    "pcc_matrices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6706f0",
   "metadata": {},
   "source": [
    "## run stat tests on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f6a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest(data0, data1):\n",
    "    stat, p_value = stats.ttest_ind(data0, data1, axis=0, equal_var=False)\n",
    "    return stat, p_value\n",
    "\n",
    "def analyze_group_differences(data, labels, stat_func = ttest):\n",
    "    groups = np.unique(labels)\n",
    "    group_data = [data[labels == g] for g in groups]\n",
    "\n",
    "    stat, p_value = stat_func(group_data[0], group_data[1])\n",
    "    p_thresh = (p_value < 0.05).astype(int)\n",
    "\n",
    "    # # permutation-based p-values\n",
    "    # n_perm = 5000\n",
    "    # rng = np.random.RandomState(42)\n",
    "    # all_data = np.concatenate(group_data, axis=0)\n",
    "    # n1 = group_data[0].shape[0]\n",
    "    # perm_stats = np.empty((n_perm,) + stat.shape)\n",
    "\n",
    "    # for i in range(n_perm):\n",
    "    #     idx = rng.permutation(all_data.shape[0])\n",
    "    #     g0 = all_data[idx[:n1]]\n",
    "    #     g1 = all_data[idx[n1:]]\n",
    "    #     tperm, _ = stats.ttest_ind(g0, g1, axis=0, equal_var=False)\n",
    "    #     perm_stats[i] = tperm\n",
    "\n",
    "    # perm_p = (np.sum(np.abs(perm_stats) >= np.abs(stat), axis=0) + 1) / (n_perm + 1)\n",
    "    # perm_p[np.isnan(stat)] = np.nan\n",
    "    # p_value_perm = perm_p\n",
    "\n",
    "    return stat, p_value, p_thresh\n",
    "\n",
    "def plot_heatmap(matrix, ax, cmap='bwr', vmin=None, vmax=None, guides_color='k'):\n",
    "    cax = ax.imshow(matrix, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    ax.set_xticks(centers)\n",
    "    ax.set_xticklabels(group_names)\n",
    "    ax.set_yticks(centers)\n",
    "    ax.set_yticklabels(group_names)\n",
    "    for boundary in boundaries:\n",
    "        ax.axhline(boundary, color=guides_color, linewidth=1.5)\n",
    "        ax.axvline(boundary, color=guides_color, linewidth=1.5)\n",
    "    return cax\n",
    "\n",
    "def plot_stats(stat, p_vals, p_thresh):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(14, 5))\n",
    "    cax1 = plot_heatmap(stat, ax[0], vmin=-5, vmax=5)\n",
    "    ax[0].set_title(\"T-statistics\")\n",
    "    fig.colorbar(cax1, ax=ax[0], fraction=0.045)  \n",
    "\n",
    "    cax2 = plot_heatmap(p_vals, ax[1], vmin=0, vmax=1, cmap='inferno_r')\n",
    "    ax[1].set_title(\"p-values\")\n",
    "    fig.colorbar(cax2, ax=ax[1], fraction=0.045)  \n",
    "\n",
    "    cax3 = plot_heatmap(p_thresh, ax[2], vmin=0, vmax=1, cmap='inferno')\n",
    "    ax[2].set_title(\"Significant p < 0.05\")\n",
    "    cbar3 = fig.colorbar(cax3, ax=ax[2], fraction=0.045)\n",
    "    cbar3.set_ticks([0, 1])\n",
    "    cbar3.set_ticklabels(['False', 'True'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d47978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot means\n",
    "\n",
    "groups = np.unique(diagnoses)\n",
    "group_data = [pcc_matrices[diagnoses == g] for g in groups]\n",
    "means = [np.mean(gd, axis=0) for gd in group_data]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "cax = plot_heatmap(means[0], ax[0], cmap='bwr', vmin=-1, vmax=1)\n",
    "fig.colorbar(cax, ax=ax[0], fraction=0.045)\n",
    "ax[0].set_title(\"Patients Mean PCC\")\n",
    "cax = plot_heatmap(means[1], ax[1], cmap='bwr', vmin=-1, vmax=1)\n",
    "fig.colorbar(cax, ax=ax[1], fraction=0.045)\n",
    "ax[1].set_title(\"Controls Mean PCC\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd8f082",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p_value, p_thresh = analyze_group_differences(pcc_matrices, diagnoses)\n",
    "plot_stats(stat, p_value, p_thresh)\n",
    "\n",
    "# compute True rate in p_thresh; I will use it as a threhold for ML feature importance selection\n",
    "r_significant = np.sum(p_thresh)/np.size(p_thresh)\n",
    "print(f\"Proportion of significant connections: {r_significant:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd7b9c",
   "metadata": {},
   "source": [
    "## use ML to find predictive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa5914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def forest_features(X, y, n_estimators=2000, threshold=0.5):\n",
    "    C = X.shape[1]\n",
    "    tril_indices = np.tril_indices(X.shape[1], k=-1)\n",
    "    X = X[:, tril_indices[0], tril_indices[1]]\n",
    "    n_samples = X.shape[0]\n",
    "    X = X.reshape(n_samples, -1)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators)\n",
    "    clf.fit(X, y)\n",
    "    importances = clf.feature_importances_\n",
    "\n",
    "    # reshape importances back to matrix form\n",
    "    full_importances = np.zeros((C, C))\n",
    "    full_importances[tril_indices] = importances\n",
    "    full_importances = full_importances + full_importances.T\n",
    "    importances = full_importances\n",
    "\n",
    "    # get a mask of top k% importances\n",
    "    # topk_importances = np.percentile(importances, 100 * threshold)\n",
    "    topk_importances = np.percentile(importances, 100 * (1-threshold))\n",
    "    mask = importances > topk_importances\n",
    "\n",
    "    return importances, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef38ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances, importance_mask = forest_features(pcc_matrices, diagnoses, threshold=r_significant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(14, 5))\n",
    "cax = plot_heatmap(importances, ax[0], cmap='inferno', guides_color='white')\n",
    "fig.colorbar(cax, ax=ax[0], fraction=0.045)\n",
    "ax[0].set_title(\"Feature Importances\")\n",
    "cax = plot_heatmap(np.log(importances), ax[1], cmap='inferno', guides_color='white')\n",
    "fig.colorbar(cax, ax=ax[1], fraction=0.045)\n",
    "ax[1].set_title(\"Log Feature Importances\")\n",
    "\n",
    "cax = plot_heatmap(importance_mask, ax[2], cmap='inferno')\n",
    "ax[2].set_title(\"Significant Features\")\n",
    "cbar3 = fig.colorbar(cax, ax=ax[2], fraction=0.045)\n",
    "cbar3.set_ticks([0, 1])\n",
    "cbar3.set_ticklabels(['False', 'True'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ab2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot p_thresh and importance_mask, and AND matrix\n",
    "fig, ax = plt.subplots(1, 3, figsize=(14, 5))\n",
    "cax1 = plot_heatmap(p_thresh, ax[0], cmap='inferno')\n",
    "sp_p = np.sum(p_thresh)/np.size(p_thresh)\n",
    "ax[0].set_title(f\"p < 0.05 ({int(sp_p*100)}% density)\")\n",
    "cbar1 = fig.colorbar(cax1, ax=ax[0], fraction=0.045)\n",
    "cbar1.set_ticks([0, 1])\n",
    "cbar1.set_ticklabels(['False', 'True'])\n",
    "\n",
    "cax2 = plot_heatmap(importance_mask, ax[1], cmap='inferno')\n",
    "sp_rf = np.sum(importance_mask)/np.size(importance_mask)\n",
    "ax[1].set_title(f\"Random Forest results ({int(sp_rf*100)}% density)\")\n",
    "cbar2 = fig.colorbar(cax2, ax=ax[1], fraction=0.045)\n",
    "cbar2.set_ticks([0, 1])\n",
    "cbar2.set_ticklabels(['False', 'True'])\n",
    "\n",
    "and_matrix = p_thresh & importance_mask\n",
    "sp_and = np.sum(and_matrix)/np.size(and_matrix)\n",
    "cax3 = plot_heatmap(and_matrix, ax[2], cmap='inferno')\n",
    "ax[2].set_title(f\"AND Matrix ({int(sp_and*100)}% density)\")\n",
    "cbar3 = fig.colorbar(cax3, ax=ax[2], fraction=0.045)\n",
    "cbar3.set_ticks([0, 1])\n",
    "cbar3.set_ticklabels(['False', 'True'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf690b7",
   "metadata": {},
   "source": [
    "### compare classification scores with different subsets of predictive features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe0f327",
   "metadata": {},
   "source": [
    "# More statistics\n",
    "\n",
    "I will use pyspi to compute statistics. Check their docummentation: https://time-series-features.gitbook.io/pyspi/installing-and-using-pyspi/usage/walkthrough-tutorials/getting-started-a-simple-demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d961540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspi.calculator import Calculator\n",
    "import dill\n",
    "\n",
    "for i, test_subject in enumerate(timeseries):\n",
    "\n",
    "    if i < 222:\n",
    "        continue\n",
    "    test_subject = test_subject.T # pyspi calc expects data in shape [chanels, time]\n",
    "    calc = Calculator(dataset=test_subject, configfile='./custom_config.yaml') # instantiate the calculator object\n",
    "\n",
    "    calc.compute()\n",
    "\n",
    "    # save and load pickle\n",
    "\n",
    "    save_path = \"/Users/ppopov1/adm-proj/data/pyspi/\"\n",
    "    dill.dump(calc, open(save_path+f\"pyspi_calc_{i:03d}.pkl\", 'wb'))\n",
    "\n",
    "    # with open(save_path+f\"pyspi_calc_{i:03d}.pkl\", 'rb') as f:\n",
    "    #     calc_load = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for spi_type in calc.spis:\n",
    "    spi_data = calc.table[\"cov_EmpiricalCovariance\"].to_numpy()\n",
    "    # get values off-diagonal\n",
    "    off_diag = spi_data[np.triu_indices_from(spi_data, k=1)]\n",
    "    nans_off_diag = np.isnan(off_diag).any()\n",
    "    if not nans_off_diag:\n",
    "        counter += 1\n",
    "    else:\n",
    "        print(F\"{spi_type} data has nans off-diagonal: {nans_off_diag}\")\n",
    "\n",
    "print(F\"Number of SPI types without nans off-diagonal: {counter} out of {len(calc.spis)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64663d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc.table[\"cov_EmpiricalCovariance\"]\n",
    "calc.table[\"cov_EmpiricalCovariance\"].to_numpy().shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
